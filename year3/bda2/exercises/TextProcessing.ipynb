{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text With Python\n",
    "## Preprocess Data For Classification and Regression\n",
    "### Load Initial Dataset with Multiple Documents of Text\n",
    "\n",
    "Data is based on the Imdb movie review database. The data has been stored locally. If you want to fetch the data you can find it here: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz . The data is organized in two subfolders train and test each of which contains folders with positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"../../P/datasets/imdb/aclImdb/train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch object is dictionary like and consists of the keys\n",
    "* data An array of data to learn from\n",
    "* target The associated classifications labels\n",
    "* target_names the meaning of the labels\n",
    "* feature_names the meaning of the features (optional)\n",
    "* DESCR the full description of the dataset\n",
    "* filename the physical location of iris csv dataset\n",
    "\n",
    "Heres what the first entries look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "b'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'\n",
      "2\n",
      "neg\n",
      "None\n",
      "['../../P/datasets/imdb/aclImdb/train/unsup\\\\5384_0.txt'\n",
      " '../../P/datasets/imdb/aclImdb/train/unsup\\\\48493_0.txt'\n",
      " '../../P/datasets/imdb/aclImdb/train/unsup\\\\20575_0.txt' ...\n",
      " '../../P/datasets/imdb/aclImdb/train/unsup\\\\25853_0.txt'\n",
      " '../../P/datasets/imdb/aclImdb/train/unsup\\\\26711_0.txt'\n",
      " '../../P/datasets/imdb/aclImdb/train/unsup\\\\48943_0.txt']\n"
     ]
    }
   ],
   "source": [
    "print(reviews_train.keys())\n",
    "\n",
    "print(reviews_train.data[0])\n",
    "print(reviews_train.target[0])\n",
    "print(reviews_train.target_names[0])\n",
    "print(reviews_train.DESCR)\n",
    "print(reviews_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the .data key and the .target key we can create a list of training data with their related class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 75000\n",
      "type of y_train: <class 'numpy.ndarray'>\n",
      "length of y_train: 75000\n"
     ]
    }
   ],
   "source": [
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "\n",
    "print(\"type of y_train: {}\".format(type(y_train)))\n",
    "print(\"length of y_train: {}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the data by removing the html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect which categories we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'unsup']\n"
     ]
    }
   ],
   "source": [
    "print(reviews_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Unsup' are unsupported reviews (for which no categorization exists). We should ignore these. Care must be taken when removing the respective entries from the data. We need to identify the \"unsup\" class positions in the y_train array first and then remove them in y_train and the respective indices from text_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(y_train == 2)\n",
    "y_train = np.delete(y_train, idx[0])\n",
    "text_train = np.delete(text_train, idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(text_train))\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the data as bag of words\n",
    "Now we create a bag of words, i.e. for each word in the documents, we count their occurence on a per document level. Hence, each new word adds a dimension to our feature vector, and each new document adds a new line in our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "imdb_bag_of_words = vectorizer.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer now contains all the words that have been found. The matrix has now the dimension (num_words x num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 74849\n",
      "bag_of_words: <25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n",
      "word: dan\t count 16414 \n",
      "word: katzir\t count 36171 \n",
      "word: has\t count 29999 \n",
      "word: produced\t count 51851 \n",
      "word: wonderful\t count 73496 \n",
      "word: film\t count 24536 \n",
      "word: that\t count 66322 \n",
      "word: takes\t count 65348 \n",
      "word: us\t count 70492 \n",
      "word: on\t count 46916 \n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vectorizer.vocabulary_)))\n",
    "print(\"bag_of_words: {}\".format(repr(imdb_bag_of_words)))\n",
    "for x in list(vectorizer.vocabulary_)[0:10]:\n",
    "    print (\"word: {}\\t count {} \".format(x,  vectorizer.vocabulary_[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the features, we see, that there are features, that are not very likely to give us valuable informatio. These contain numbers, time of day representations and stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "First 20 features:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "Features 20010 to 20030:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "Every 2000th feature:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bÃªte', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature space is vast with nearly 75000 dimensions. Hence we should try to reduce the number of dimensions by:\n",
    "* use only words that have a mimimum occurence in all documents (minimal document frequency) min_df\n",
    "* remove stop words (like 'a', 'and', 'the') as they don't give valuable information for classification and/or \n",
    "* remove words that occur in may documents (maximum document frequency) max_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=\"english\", max_df=1000).fit(text_train)\n",
    "X_train = vect.transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 26630\n",
      "X_train size: <25000x26630 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 1399128 stored elements in Compressed Sparse Row format>\n",
      "word: dan\t count 5964 \n",
      "word: produced\t count 18384 \n",
      "word: roller\t count 20154 \n",
      "word: coaster\t count 4572 \n",
      "word: ride\t count 19969 \n",
      "word: romance\t count 20163 \n",
      "word: troubles\t count 24544 \n",
      "word: surrounding\t count 23234 \n",
      "word: modern\t count 15397 \n",
      "word: israel\t count 12698 \n",
      "Number of features: 26630\n",
      "First 20 features:\n",
      "['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '100', '1000', '100th', '101', '102', '103', '104']\n",
      "Features 20010 to 20030:\n",
      "['riley', 'ring', 'ringer', 'ringing', 'ringmaster', 'ringo', 'rings', 'ringu', 'ringwald', 'rio', 'rios', 'riot', 'riotous', 'riots', 'rip', 'ripe', 'ripley', 'ripoff', 'ripoffs', 'ripped']\n",
      "Every 2000th feature:\n",
      "['00', 'bananas', 'characterised', 'dapper', 'endurance', 'germany', 'inauthentic', 'livesey', 'negotiate', 'pos', 'righteousness', 'sod', 'ticking', 'werewolves']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"X_train size: {}\".format(repr(X_train)))\n",
    "for x in list(vect.vocabulary_)[0:10]:\n",
    "    print (\"word: {}\\t count {} \".format(x,  vect.vocabulary_[x]))\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescaling the data using term frequencey inverse document frequency\n",
    "Here, term frequency is the number of occurences of a term (word) $t$ in a document $d$. \n",
    "\n",
    "$\\operatorname{tf}(t, d) = f_{t, d}$ \n",
    "\n",
    "Sometimes tf gets normalized to the length of $d$\n",
    "The inverse document frequency idf is a measure on the amount of information a term t carries. Rare occurences of t leads to a high amount of information common occurence to a low amount of information. The idf is computed as \n",
    "\n",
    "$\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$\n",
    "\n",
    "where $n$ is the total number of documents and $\\text{df}(t)$ is the number of documents that contain the term $t$. Hence, the tf-idf is the product of the two terms:\n",
    "\n",
    "$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\cdot \\text{idf(t)}$\n",
    "\n",
    "scikit-learn supports this in the `TfidfTransformer`, when using the following parameters: `TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)`. Refer to the scikit documentation for the parameter sets and how this changes the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of tf-idf to Imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000, norm=None)\n",
    "X_idf_train = tfidf_vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word now has an tf-idf value when it occurs in a document or zero, if it does not occur in a document (row of the matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n",
      " 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'avoided'\n",
      " 'fond' 'stinker' 'emphasis' 'commented' 'realizing' 'disappoint'\n",
      " 'downhill' 'inane']\n",
      "tf-idf values:\n",
      "[6.0704253  6.24386918 6.2668587  6.27464084 6.32265006 6.38173897\n",
      " 6.39047265 6.39928328 6.42619074 6.44453988 6.4822802  6.49194211\n",
      " 6.49194211 6.49194211 6.50169829 6.51155059 6.53155125 6.53155125\n",
      " 6.55196012 6.56232291]\n",
      "Features with highest tfidf: \n",
      "['roy' 'coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'macarthur'\n",
      " 'taker' 'vargas' 'jesse' 'basket' 'dominick' 'victor' 'bridget'\n",
      " 'victoria' 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n",
      "tf-idf values:\n",
      "[178.44626553 180.00910673 184.13677468 185.30580621 186.69823268\n",
      " 188.84578956 190.11881797 190.11881797 190.74952846 192.19175433\n",
      " 192.31709746 199.03905035 200.53489816 200.82037452 204.27548969\n",
      " 205.36805594 210.46552255 216.97960963 220.32008782 247.74660926]\n"
     ]
    }
   ],
   "source": [
    "# find maximum value for each of the features over dataset:\n",
    "max_value = X_idf_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "feature_names = np.array(tfidf_vect.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"tf-idf values:\\n{}\".format(max_value[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))\n",
    "print(\"tf-idf values:\\n{}\".format(max_value[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawbacks of the current approach\n",
    "While tf-idf (the `TfidfVectorizer` is a `CountVectorizer` followed by a `TfidfTransformer`) gives a proper numerical representation on a vocabulary and a set of documents, it has an important limitation. As the vectorization approach considers only on word at a time, the semantic of a sentence is ignored which leads to:\n",
    "\"this is good, not bad\" and \"this is bad, not good\" to have the same representation in their vectorized form.\n",
    "\n",
    "To cope with that we can consider more than one word at a time. A combination of n words is called a n-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words and tf-idf with n-Grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000, ngram_range = (1,3), norm=None)\n",
    "X_idf_train = tfidf_vect.fit_transform(text_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 82833)\n",
      "Vocabulary size: 82833\n"
     ]
    }
   ],
   "source": [
    "print(X_idf_train.toarray().shape)\n",
    "print(\"Vocabulary size: {}\".format(len(tfidf_vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative approach using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# code partially taken from MÃ¼ller & Guido \"Introduction to Machine Learning with Python\"\n",
    "# load spacy language model\n",
    "en_nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# create a custom tokenizer using the SpaCy document processing pipeline\n",
    "# (now using our own tokenizer)\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# define a count vectorizer with the custom tokenizer\n",
    "lemma_vect = TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000, norm=None, tokenizer=custom_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Els\\Anaconda3\\envs\\ogni\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-PRON-', 'make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lemma.shape: (25000, 21395)\n"
     ]
    }
   ],
   "source": [
    "# transform text_train using CountVectorizer with lemmatization\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (25000, 26630)\n"
     ]
    }
   ],
   "source": [
    "# standard CountVectorizer for reference\n",
    "vect = TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000, norm=None).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, the dimensionality of the feature space is high (esp. when adding n-Grams to it). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of text\n",
    "## Classification with k-NN Classifier and tf-idf using n-Grams\n",
    "`TfidfVectorizer` and `CountVectorizer` both support the parameter `ngram_range(min_n, max_n)`. So we can try out a simple classifier (e.g. k-NN) and check the performance for different n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000), KNeighborsClassifier())\n",
    "print(pipe.get_params().keys())\n",
    "# running the grid-search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': [3, 5, 7, 9],\n",
    "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5,  n_jobs=8, verbose=10)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 4).T\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "ax = sns.heatmap(scores, annot=True, cmap=\"viridis\", xticklabels=param_grid['kneighborsclassifier__n_neighbors'], yticklabels=param_grid[\"tfidfvectorizer__ngram_range\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Random Forest Classifier and n-Grams\n",
    "Using a different classifier leads to better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000), RandomForestClassifier())\n",
    "print(pipe.get_params().keys())\n",
    "# running the grid-search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "param_grid = {'randomforestclassifier__n_estimators': [47, 81, 101],\n",
    "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5,  n_jobs=8, verbose=10)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 4).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "sns.set()\n",
    "ax = sns.heatmap(scores, annot=True, cmap=\"viridis\", xticklabels=param_grid['randomforestclassifier__n_estimators'], yticklabels=param_grid[\"tfidfvectorizer__ngram_range\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Support Vector Classifications and n-Grams\n",
    "Note: This will take approx. 70-90 minutes to train on an quad core machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, stop_words=\"english\", max_df=1000), SVC(gamma='auto'))\n",
    "print(pipe.get_params().keys())\n",
    "# running the grid-search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "param_grid = {'svc__C': [0.1, 1, 10],\n",
    "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5,  n_jobs=8, verbose=10)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "sns.set()\n",
    "ax = sns.heatmap(scores, annot=True, cmap=\"viridis\", xticklabels=param_grid['svc__C'], yticklabels=param_grid[\"tfidfvectorizer__ngram_range\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that more sophisticated classifiers, which often are considered to be superior to simple classifiers (like Decisionn Trees), are not performing too well for text classification. The reason is, that the feature vector is very high dimensional **and** sparse. This is known as the \"curse of dimensionality\". Here we need alternative approaches to reduce the sparsity and dimensionality --> Data Science II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 2020 Ingo Elsen (FH Aachen University of Applied Sciences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
